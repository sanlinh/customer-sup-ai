# Self-Learning Visa AI Assistant ğŸ§­

A backend-only **self-learning AI assistant** for visa customer support, built for the **Issa Compass Vibe Hackathon**.

This service generates human-like consultant replies, learns from real historical conversations, and **only improves itself when performance measurably increases**.

---

## ğŸš€ Live Demo

**Hosted API:**

```
https://customer-sup-ai-production.up.railway.app
```

**Health Check:**

```bash
curl https://customer-sup-ai-production.up.railway.app/health
```

---

## ğŸ§  What This System Does

### 1. Human-like Replies (Not â€œAI-soundingâ€)

* Generates responses based on **real consultant conversations**
* Uses **retrieval-augmented prompting (RAG-style)** to ground tone and structure
* Avoids legal guarantees, stays concise, friendly, and calm

### 2. Self-Learning (Safely)

* Learns from historical conversations (`conversations.json`)
* Automatically proposes prompt improvements
* **Applies updates only if holdout performance improves**
* Prevents prompt drift via gated learning

### 3. Fully Measurable

* Built-in evaluation framework
* Prompt versioning + diffs
* Reproducible improvements via cURL (no UI required)

---

## ğŸ—ï¸ Architecture Overview

```
Client / Tester
   |
   |  HTTP (cURL / API)
   v
Flask API (Railway)
   |
   â”œâ”€ /generate-reply        â†’ AI reply (with RAG grounding)
   â”œâ”€ /train-from-history    â†’ Gated self-learning
   â”œâ”€ /evaluate              â†’ Quantitative scoring
   â”œâ”€ /prompt-diff/latest    â†’ Show how the AI changed
   |
   v
Groq LLMs (Generation / Training / Judge)
   |
Supabase
   â”œâ”€ prompts
   â”œâ”€ prompt_versions
   â”œâ”€ eval_runs
   â””â”€ eval_samples
```

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€ app.py                # Flask API (core logic)
â”œâ”€ dataset_builder.py    # Build training examples from conversations
â”œâ”€ prompt_builder.py     # Base system prompt
â”œâ”€ eval_and_rag.py       # Evaluation + scoring logic
â”œâ”€ conversations.json    # Sample real conversations
â”œâ”€ requirements.txt
â”œâ”€ Procfile              # Railway startup (gunicorn)
â”œâ”€ .env.example
â””â”€ README.md
```

---

## ğŸ”‘ Environment Variables

Create a `.env` file locally, or set these in Railway **Variables**:

```env
SUPABASE_URL=
SUPABASE_API_KEY=

GROQ_API_KEY=

PROMPT_NAME=visa_assistant_v1
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_TRAIN_MODEL=llama-3.1-8b-instant
```

---

## â–¶ï¸ Run Locally

```bash
pip install -r requirements.txt
python app.py
```

API will run at:

```
http://127.0.0.1:5000
```

---

## ğŸ”Œ API Endpoints & Examples

### 1ï¸âƒ£ Health Check

```bash
curl https://customer-sup-ai-production.up.railway.app/health
```

---

### 2ï¸âƒ£ Generate AI Reply (with RAG + debug)

```bash
curl -X POST "https://customer-sup-ai-production.up.railway.app/generate-reply?debug=true&ragK=3" \
  -H "Content-Type: application/json" \
  -d '{
    "clientSequence": "I am American and currently in Bali. Can I apply from Indonesia?",
    "chatHistory": []
  }'
```

**Returns:**

* `aiReply`
* retrieved historical examples (debug mode)

---

### 3ï¸âƒ£ Evaluate Current Prompt (Holdout Set)

```bash
curl -X POST https://customer-sup-ai-production.up.railway.app/evaluate \
  -H "Content-Type: application/json" \
  -d '{ "n": 25, "split": "holdout", "notes": "baseline" }'
```

---

### 4ï¸âƒ£ Gated Self-Learning (Safe Auto-Improve)

```bash
curl -X POST https://customer-sup-ai-production.up.railway.app/train-from-history \
  -H "Content-Type: application/json" \
  -d '{
    "maxSamples": 6,
    "maxUpdates": 1,
    "gateK": 5,
    "minDelta": 0.2,
    "maxSeconds": 25
  }'
```

âœ”ï¸ Prompt is updated **only if performance improves**.

---

### 5ï¸âƒ£ View Prompt Diff (Transparency)

```bash
curl https://customer-sup-ai-production.up.railway.app/prompt-diff/latest
```

Shows exactly **what changed and why**.

---

### 6ï¸âƒ£ Re-Evaluate After Training

```bash
curl -X POST https://customer-sup-ai-production.up.railway.app/evaluate \
  -H "Content-Type: application/json" \
  -d '{ "n": 25, "split": "holdout", "notes": "after training" }'
```

---

## ğŸ§ª Why This Is â€œSelf-Learningâ€ (Not Just Prompt Editing)

* Uses **labeled historical data**
* Proposes prompt edits via a dedicated editor prompt
* Runs **quantitative evaluation**
* Applies updates **only if metrics improve**
* Stores every version + diff for auditability

This mirrors real-world **ML system iteration**, not just prompt tweaking.

---

## ğŸ§‘â€ğŸ’» Tech Stack

* **Backend:** Python, Flask
* **LLMs:** Groq (LLaMA family)
* **Database:** Supabase (Postgres)
* **Deployment:** Railway + Gunicorn
* **Evaluation:** LLM-as-judge + holdout splits

---
